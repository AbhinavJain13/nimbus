################################################################################
#
# Settings for using the "pilot" resource locator.
#
# A set of VMM resources may be managed entirely by the workspace service or
# alternatively, using the pilot, integrated with a site's resource manager
# (such as PBS).
#
# This way a dual use grid cluster can be achieved: regular grid jobs can run
# on the VMM node with no guest VMs, unless the node is at that time allocated
# to the workspace service. The site resource manager maintains full control
# over the cluster and does not need to be modified.
#
# NOTE: To enable the pilot you need to run:
#       "cp other/resource-locator-pilot.xml other/resource-locator-ACTIVE.xml"
#
# NOTE: For notifications to be meaningful, reasonable clock sync between all
#       nodes is important (use NTP for example)
#
# NOTE: Logs are set to be delivered to the workspace service's persistence
#       directory (something like "var/nimbus/pilot-logs")
#
################################################################################





################################################################################
#
# Configurations for HTTP Digest access authentication based notifications.
#
################################################################################

# host:port for HTTP based notifications.  This must be an address or hostname
# that is recognized on the VMM nodes.

contact.socket=1.2.3.4:41999



################################################################################
#
# General settings
#
################################################################################

# The path to the pilot program on the VMM nodes:

pilot.path=/opt/workspacepilot.py


# The maximum available memory for a guest on the VMM nodes:

memory.maxMB=2048



################################################################################
#
# LRM specific settings (Torque)
#
# This version only supports starting the pilot with Torque (tested with
# version 2.1.8 and 2.2.1).  Contact the workspace-user mailing list for
# other requirements, adapting to other LRMs should be straightforward if
# they support typical job cancellation semantics.
#
################################################################################

# Path to job submission program (if program is on the container account's
# PATH, then this does not need to be an absolute path):

pbs.submit.path=qsub


# Path to job deletion program (if program is on the container account's
# PATH, then this does not need to be an absolute path):

pbs.delete.path=qdel


# Processors per node, right now this should be set to be the maximum processors
# on each cluster node.  If it set too high, pilot job submissions will fail.
# If it is set too low, the pilot may end up not being the only LRM job on the
# node at a time and that is unpredictable/unsupported right now.

pbs.ppn=2


# If the pilot job should be submitted to a special queue/server, configure
# that here.  For Torque, this value is sent as the "-q destination" parameter,
# and can take one of three forms: "queue", "@server", or "queue@server".
# If this configuration is empty or missing, the pilot job is run with defaults
# for the submitting user (the container).

pbs.destination=


# The grace period (seconds) between LRM's SIGTERM and SIGKILL.  Can be zero,
# but this gives no time for VWS to cleanly kill the running workspaces in the
# slot in the case where the slot job is preempted.

pbs.grace=8


# Optional, extra node properties needed in the submission.  (for example you
# might label nodes in the PBS pool with a string to distinguish them, like
# 'xen' perhaps).

pbs.extra.properties=


# Optional, if configured this is prepended to the pilot exe invocation if
# nodes needed are greater than one.  Torque uses pbsdsh for this.

pbs.multijob.prefix=pbsdsh -u

