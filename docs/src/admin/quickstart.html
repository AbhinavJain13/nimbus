m4_include(/mcs/m4/worksp.lib.m4)
_NIMBUS_HEADER(2.3 Admin Quickstart)
_NIMBUS_HEADER2(n,n,y,n,n,n,n)
_NIMBUS_LEFT2_COLUMN
_NIMBUS_LEFT2_ADMIN_SIDEBAR(n,y,n,n,n)
_NIMBUS_LEFT2_COLUMN_END
_NIMBUS_CENTER2_COLUMN
_NIMBUS_TP2_2_DEPRECATED

<h2>Nimbus 2.3 Admin Quickstart</h2>

<p>
    The quickstart assumes the services node you are working on is Linux or
    some UNIX variant (such as OSX perhaps).  The VMM nodes are required to
    be Linux.  Most testing is carried out on Linux currently.
</p>

<p>
    There are two system accounts involved:
</p>
<ul>
    <li>
        <p>
            <b>privileged account</b> - Pick a privileged account to run the
            remote client facing service.  A privileged account on the VMM nodes
            is necessary as well.
            This guide assumes the privileged account on the service node and
            VMM nodes have the same name but that is not strictly necessary.
        </p>
        <p>
            In this guide (especially in the command samples) we will refer
            to an account of this type named <i>nimbus</i> with terminal
            prompts like "<b>nimbus $</b>"
        </p>
    </li>
    <li>
        <p>
            <b>superuser account</b> - The root account is necessary to install
            dependencies on the VMM nodes (Xen/KVM, ebtables, etc.) and also to
            install the Nimbus agent that lives on the VMM nodes (workspace
            control).
        </p>
        <p>
            In this guide (especially in the command samples) we will refer
            to an account of this type named <i>root</i> with terminal
            prompts like "<b>root #</b>"
        </p>
    </li>
</ul>

<p>
    The quickstart is broken up into these steps:
</p>

<ul style="list-style: disc">
    <li>
        <p>
            <a href="#part-I">Part I</a> - Install/verify dependencies
        </p>
        <ul style="list-style: disc">
            <li>I.A. <a href="#part-Ia">Service container</a></li>
            <li>I.B. <a href="#part-Ib">Xen</a></li>
            <li>I.C. <a href="#part-Ic">Misc. libraries for the VMM nodes</a></li>
        </ul>
    </li>
    <li>
        <p>
            <a href="#part-II">Part II</a> -
            Install/verify the Nimbus service package
        </p>
        <ul style="list-style: disc">
            <li>II.A. <a href="#part-IIa">Download and install</a></li>
            <li>II.B. <a href="#part-IIb">Necessary configurations</a></li>
            <li>II.C. <a href="#part-IIc">Test call</a></li>
        </ul>
    </li>
    <li>
        <p>
            <a href="#part-III">Part III</a> - Install/verify workspace-control
        </p>
        <ul style="list-style: disc">
            <li>III.A. <a href="#part-IIIa">Download and install</a></li>
            <li>III.B. <a href="#part-IIIb">Necessary configurations</a></li>
            <li>III.C. <a href="#part-IIIc">Testing</a></li>
        </ul>
    </li>
    <li>
        <p>
            <a href="#part-IV">Part IV</a> - End to end test
        </p>
    </li>
</ul>

<br />





<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->





<a name="part-I"> </a>
<h2>Part I: Install/verify dependencies</h2>

<ul>
    <li style="list-style: disc">I.A. <a href="#part-Ia">Service container</a></li>
    <li>I.B. <a href="#part-Ib">Xen/KVM and libvirt</a></li>
    <li>I.C. <a href="#part-Ic">Misc. libraries for the VMM nodes</a></li>
</ul>

<a name="auto-container"> </a>
<a name="part-Ia"> </a>
<h3>I.A. Service container _NAMELINK(part-Ia) _NAMELINK(auto-container)</h3>
<p>
    Currently Nimbus is deployed into a Globus 4.0.x Java container which is a
    system built around the <a href="http://ws.apache.org/axis/">Apache Axis</a>
    engine.
</p>

<p>
    To get you started with this very quickly,
    we provide something called the <b><i>AutoContainer</i></b> which is a
    complete Globus Java web services core environment.  It comes with a setup
    program that configures everything you need to <b>get a secure web services
    container running from scratch in about a minute</b>.
</p>
<p>
    This includes creating a new certificate authority, creating a new host
    certificate, creating a new user certificate(s), and adjusting relevant
    configuration files. 
</p>

<p><i>Other (more time consuming) approaches:</i></p>
<ul>
    <li>
        If you are an intermediate Globus user, you may want to check out
        <a href="reference.html#manual-container">this section</a> of the
        reference manual for a more manual approach.
    </li>
    <li>
        If you are a veteran Globus user, you will probably already have a Java
        web services environment set up with security somewhere and can
        skip this section altogether.
    </li>
</ul>

<a name="acquire-auto-container"> </a>
<h4>* Acquire the AutoContainer: _NAMELINK(acquire-auto-container)</h4>

<p>
    On the <a href="_NIMBUS_WEBSITE/downloads/">downloads</a> page is a recent version of the
    AutoContainer.
</p>

<a name="run-auto-container"> </a>
<h4>* Run the AutoContainer: _NAMELINK(run-auto-container)</h4>

<p>
    In the following steps, make sure you've expanded the directory to the
    place you want to leave it (absolute paths will be used for some
    configurations).
</p>

_EXAMPLE_CMD_BEGIN
tar xzf autocontainer-GT4.0.8-v1.0.tar.gz
_EXAMPLE_CMD_END

_EXAMPLE_CMD_BEGIN
cd autocontainer
_EXAMPLE_CMD_END

_EXAMPLE_CMD_BEGIN
./bin/setup-container.sh
_EXAMPLE_CMD_END

<p>
    That's it.  The program will instruct you on any other necessary steps.
</p>
<p>
    You'll end up being directed to run a test command whose output will look
    something like this if everything is set up:
</p>
_EXAMPLE_CMD_BEGIN
/somewhere/autocontainer/bin/test-container.sh
_EXAMPLE_CMD_END
<div class="screen"><pre>
Got notification with value: 3
Counter has value: 3
Got notification with value: 13</pre>
</div>


<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<a name="part-Ib"> </a>
<h3>I.B. Xen/KVM and libvirt _NAMELINK(part-Ib)</h3>
<p>
    The workspace service manages <a href="http://www.xen.org">Xen</a>
    or <a href="http://www.linux-kvm.org">KVM</a> VMs.  It interfaces with both
    of these systems using a library called 
    <a href="http://libvirt.org/">libvirt</a>.
</p>
<p>
    The workspace service itself does not need to run on a VMM node.  You
    must pick VMM nodes to install Xen or KVM on, the workspace service will
    manage these.  For the quickstart, pick just one node to install and test
    with.
    When all goes well, add more into the pool of VMMs the workspace service
    can send work to.
</p>
<p>
    You need to install Xen/KVM and get a basic VM running with bridged
    networking.
</p>
<p>
    Nimbus has been most heavily tested with the Xen 3.1 series although it
    is known to be working with 3.2 (this may require the "tap:aio"
    configuration
    in workspace-control's "xen.conf" configuration file).  There is no
    information yet about Nimbus on the new/upcoming Xen 3.3 series.  Nimbus
    has also been tested with QEMU/KVM 0.11.0
</p>

<p>
    You can actually put off the whole VMM step until later if you just want to evaluate
    the Nimbus clients and services:  right out of the box Nimbus makes "fake"
    calls to workspace control (which is the agent that actually manages things
    on the VMM nodes).  This allows the service side to be tested independently
    of the VMM.  You "join them up" by turning off the "fake" switch when you're
    ready to do end to end testing.
</p>

<a name="acquire-install-vmm"> </a>
<h4>* Acquire and install VMM: _NAMELINK(acquire-install-xen)</h4>

<p>
    Your Linux distribution might have good support for Xen or KVM
    (Debian/Ubuntu, RedHat, Gentoo, SUSE, etc.) in which case it is best to
    defer to distribution specific notes about how to get it up and running
    (with libvirt and bridged networking).
</p>
<p>
    If your Linux distribution doesn't support Xen or KVM well or you are having
    some inexplicable problem, you can always try installing from source
    or binary, starting at <a href="http://libvirt.org/">libvirt.org</a> (and either <a href="http://xen.org/">xen.org</a> or <a href="http://www.linux-kvm.org">linux-kvm.org</a>).
</p>

<a name="test-xen"> </a>
<h4>* Test Xen: _NAMELINK(test-xen)</h4>

<p>
    You can test Xen with any Xen VM image, mostly.  In this guide we will use a
    small and simple VM image as an example.  Look
    <a href="_SCIENCECLOUDS_WEBSITE/marketplace/#other">here</a> for some
    ideas about where to find Xen VM images.  You can also create them from
    scratch of course, if you have the time.
</p>

<p>
    In particular, we will be using a ttylinux-based image. ttylinux is a very
    small, yet functional, Linux distribution requiring only around 4 MB.
    Visit the <a href="http://www.minimalinux.org/ttylinux/">ttylinux</a>
    home page for a list of some of its many nice features.
</p>

<p>
    You can download a tarball containing the image
    <a href="_NIMBUS_WEBSITE/downloads/ttylinux-xen-libvirt.tar.gz">here</a>.
    This directory should contain the following files:
</p>
<ul>
    <li>
        ttylinux-xen.img: The partition image
    </li>
    <li>
        libvirt-ttylinux.xml: A sample libvirt configuration
    </li>
</ul>

<p>
    Take into account that the provided ttylinux image is not the exact same
    image you can download from the ttylinux home page. It is preconfigured
    to obtain a network address through DHCP, and depends on a different root
    device than a regular ttylinux image (to make the image Xen-friendlier).
</p>

<p>
    Test the image using the provided configuration file. First of all, make
    sure you replace the values of the kernel and disk parameters inside the
    file with appropriate values. In particular, you should point kernel to
    your Xen guest kernel and disk to the location of the ttylinux-xen file.
</p>

<p>
    To test the image, run the following:
</p>

_EXAMPLE_ROOTCMD_BEGIN
virsh -c "xen:///" create libvirt-ttylinux.xml
_EXAMPLE_CMD_END

<p>
    Logging in via terminal using:
</p>

_EXAMPLE_ROOTCMD_BEGIN
vncviewer localhost::5900
_EXAMPLE_CMD_END

<p>
    You should see ttylinux boot messages, followed by a login prompt. You can
    log in using user 'root' and password 'root'. You can use ifconfig to check
    that the networking interface (eth0) is correctly setup. If you do not have
    a DHCP server, you can also use ifconfig to configure eth0 with a static
    IP address. Once the network is correctly set up, you should be able to
    ping out to some other machine besides dom0 and from that other machine
    be able to ping this address. Note that you should do all this just for
    the purposes of verifying that the image works correctly.
</p>
<p>
    Keep the image configured to obtain a network address automatically via
    DHCP (even if your network doesn't have a DHCP server), as the Workspace
    Service will use DHCP to dynamically set up networking in it.
</p>
<p>
    <b>Note</b>: most Xen documentation you will find online about
    testing and troubleshooting the install
    is <i>valid</i> in this situation because you are not doing anything
    specifically related to Nimbus here.
</p>
<p>
    <b>Note</b>: make sure you read the section below about <a href="#libvirt-connection">libvirt connection strings</a>.
</p>

<a name="test-kvm"> </a>
<h4>* Test KVM: _NAMELINK(test-kvm)</h4>

<p>
    To test KVM we do not have a ready made image for you yet.  Use an image
    you have already gotten to run using "virsh -c qemu:///system"
</p>

<p>
    <b>Note</b>: make sure you read the section below about <a href="#libvirt-connection">libvirt connection strings</a>.
</p>

<a name="libvirt-connection"> </a>
<h4>* Libvirt connection: _NAMELINK(libvirt-connection)</h4>

<p>
    The workspace-control tool will be operating from a privileged UNIX account
    that is NOT root.  The best way to allow this account to interface with
    libvirt is to use UNIX domain sockets and group permissions.  See
    libvirtd.conf (usually located in the /etc directory) for details.
</p>
<p>
    The connection strings for KVM and Xen
    will usually be similar to "qemu:///system" and
    "///var/run/xend/xend-socket" respectively for this method.
</p>
<p>
    Ultimately do what is best for your deployment situation, the details can be
    found on the <a href="http://libvirt.org/uri.html">libvirt site</a>.
    Whatever connection URI you choose to use, it must be configured in
    workspace-control's "libvirt.conf" file.
</p>

<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->


<a name="part-Ic"> </a>
<h3>I.C. Misc. libraries for the VMM nodes _NAMELINK(part-Ic)</h3>
<p>
    There are a few libraries needed on the VMM nodes before workspace control
    will work.
</p>

<p>
    The <a href="http://www.isc.org/index.pl?/sw/dhcp/" target="_top">ISC
    DHCP server</a> (or DHCP server with compatible conf file)
    and <a href="http://ebtables.sourceforge.net/" target="_top">ebtables</a>
    are required to be running on each hypervisor node.
</p>

<p>
    Any recent version of each package should be compatible, the scripts
    distributed with workspace control that automate the configurations were
    tested with ISC DHCP 3.0.3 and ebtables 2.0.6 userspace tools.
</p>

<p>
    For more information on why this software is now necessary and
    how it will not interfere with a site's pre-existing DHCP server, see
    <a href="reference.html#backend-config-invm-networking">the
    network configuration details section</a> in the reference guide.
</p>

<p>
    Since these two pieces of software are relatively common, they may
    already be present on your hypervisor nodes via the package management
    system.  Check your distribution tools  for packages called
    <i>dhcp</i> (ISC DHCP server) and <i>ebtables</i>. You can also check
    for the existence of <i>/sbin/ebtables</i> or <i>/usr/sbin/ebtables</i>
    (for ebtables) and any of the following files for the DHCP server:
</p>

<ul>
    <li>
        <p>
            <i>/etc/dhcp/dhcpd.conf</i>
        </p>
    </li>

    <li>
        <p>
            <i>/etc/dhcp3/dhcpd.conf</i>
        </p>
    </li>

    <li>
        <p>
            <i>/etc/init.d/dhcpd</i>
        </p>
    </li>

    <li>
        <p>
            <i>/etc/init.d/dhcp3-server</i>
        </p>
    </li>
</ul>

<p>
    If these software packages are not installed, all major Linux distributions
    include them and you should be able to easily install them with your package
    management system.  For example, "rpm -ihv dhcp-*.rpm", "apt-get install
    dhcp", "emerge dhcp", etc.  And similarly for   ebtables.
</p>

<p>
    ebtables requires kernel support in dom0, the default Xen kernel
    includes this support.  If your dom0 kernel does not include these
    for some reason, the options to enable are under Networking ::
    Networking options :: Network packet filtering :: Bridge Netfilter
    Configuration :: Ethernet Bridge Tables
</p>

<p>
    workspace-control also requires
    <a href="http://www.courtesan.com/sudo/">sudo</a> and Python 2.3+ on all
    VMM nodes under the control of the Workspace Service.
</p>



<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->




<a name="part-Id"> </a>
<h3>I.D. SSH _NAMELINK(part-Id)</h3>
<p>
    Finally, the privileged account on the service node needs to be able to
    SSH freely to the VMM nodes (without needing a password).  And vice versa,
    the privileged account on the VMM nodes needs to be able to freely SSH
    back to the service nodes to deliver notifications.
</p>

<p>
    You will be given the opportunity to get this right later during the service
    auto-configuration steps.  The relevant security setups will be tested
    interactively, making sure you get this right.
</p>


<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->


<br />
        
<a name="part-II"> </a>
<h2>Part II: Install/verify the Nimbus service package _NAMELINK(part-II)</h2>

<ul style="list-style: disc">
    <li>II.A. <a href="#part-IIa">Download and install</a></li>
    <li>II.B. <a href="#part-IIb">Necessary configurations</a></li>
    <li>II.C. <a href="#part-IIc">Test call</a></li>
</ul>



<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<a name="part-IIa"> </a>
<h3>II.A. Download and install _NAMELINK(part-IIa)</h3>

<h4>* Retrieve and unpack:</h4>
<p>
    Grab the main Nimbus archive from the <a href="_NIMBUS_WEBSITE/downloads">downloads</a>
    page and unpack the archive:
</p>

_EXAMPLE_CMD_BEGIN
wget http://www.nimbusproject.org/downloads/nimbus-2.3.tar.gz
_EXAMPLE_CMD_END

_EXAMPLE_CMD_BEGIN
tar xzf nimbus-2.3.tar.gz
_EXAMPLE_CMD_END

<a name="set-gl"> </a>
<h4>* Set the target container: _NAMELINK(set-gl)</h4>
<p>
    Now navigate into the new directory and set the "GLOBUS_LOCATION"
    variable to the target container directory you installed above:
</p>

_EXAMPLE_CMD_BEGIN
cd nimbus-2.3
_EXAMPLE_CMD_END

_EXAMPLE_CMD_BEGIN
export GLOBUS_LOCATION=/path/to/globus_location
_EXAMPLE_CMD_END

<p>
    If you installed the AutoContainer, this would look something like this: 
</p>
_EXAMPLE_CMD_BEGIN
export GLOBUS_LOCATION=/somewhere/autocontainer/gt/ws-core-4.0.8/
_EXAMPLE_CMD_END

<p>
    Alternatively you could get that same GLOBUS_LOCATION set in your
    environment by sourcing this file:
</p>
_EXAMPLE_CMD_BEGIN
source /somewhere/autocontainer/bin/source-me.sh
_EXAMPLE_CMD_END
<p>
    (a quick examination of the <i>source-me.sh</i> script will reveal that
    there is not much to that)
</p>

<a name="build-install"> </a>
<h4>* Build and install: _NAMELINK(build-install)</h4>
<p>
    There are several options in the "bin" directory (see the toplevel
    README file).  Choosing "./bin/all-build-and-install.sh" is all you
    should need to do.
</p>

_EXAMPLE_CMD_BEGIN
./bin/all-build-and-install.sh
_EXAMPLE_CMD_END

<p>
    A log of a successful build can be seen <a href="nimbus-build.txt">here</a>.
</p>

<p>
    If you look into that output, you can see that several Nimbus components
    are built and installed by default:
</p>

<ul>
    <li>
        The Java based RM API and workspace service - VM/VMM manager
    </li>
    <li>
        Default clients
    </li>
    <li>
        WSRF frontend - a remote protocol implementation
        compatible with the default clients
    </li>
    <li>
        EC2 frontend - a remote protocol implementation
        compatible with EC2 clients
    </li>
</ul>

<p>
    For more information about what these things are, see the
    <a href="../faq.html">FAQ</a>.
</p>
<p>
    You will not be able to run against the EC2 frontend until you have
    set up the cloud configuration because it relies on users having a
    personal repository directory (out of scope of this document).
</p>









<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<a name="part-IIb"> </a>
<h3>II.B. Necessary configurations _NAMELINK(part-IIb)</h3>

<p>
    There are a few configurations that cannot have any defaults.
</p>

<p>
    We provide an <b>auto-configuration</b> program
    to gently take you through these configurations.  During the process,
    several tests will be made to verify your set up.
</p>

<p>
    This is installed by default, run the following command to get started:
</p>

_EXAMPLE_CMD_BEGIN
$GLOBUS_LOCATION/share/nimbus-autoconfig/autoconfig.sh
_EXAMPLE_CMD_END

<p>
    Otherwise you can refer to
    <a href="reference.html#manual-nimbus-basics">this section</a>
    of the reference guide to see the old instructions.
    Those instructions may shed some light on certain
    configurations as you move past the testing stage and want to know
    more about what is happening (but honestly, the best place to look
    for configuration insight is in the .conf file inline comments).
</p>


<a name="authorization"> </a>
<h4>* Authorization: _NAMELINK(authorization)</h4>
<p>
    In the beginning of the quickstart you may have edited a grid-mapfile to
    authorize a client to make a secure call to the container?  Or if you
    are using the <a href="#auto-container">AutoContainer</a>,
    the container-wide grid-mapfile can be found like so:
</p>

_EXAMPLE_CMD_BEGIN
grep gridmap $GLOBUS_LOCATION/etc/globus_wsrf_core/global_security_descriptor.xml
_EXAMPLE_CMD_END

<p>
    A grid-mapfile is basically an access control list.  It says which remote
    identities can access the container or a specific service.
</p>

<p>
    There is a grid-mapfile specific to Nimbus set up by default.
    You can make Nimbus use the master container grid-mapfile that is
    configured OR you can keep it as a specific policy for Nimbus only.
</p>
<p>
    To make it the same as the master container policy,
    edit the following file to point at the
    pre-existing grid-mapfile:
</p>

_EXAMPLE_CMD_BEGIN
nano -w $GLOBUS_LOCATION/etc/nimbus/factory-security-config.xml
_EXAMPLE_CMD_END

<p>
    To keep it unique to Nimbus, edit the following grid-mapfile and add identities.
</p>

_EXAMPLE_CMD_BEGIN
nano -w $GLOBUS_LOCATION/etc/nimbus/nimbus-grid-mapfile
_EXAMPLE_CMD_END

<p>
    If you used the AutoContainer, you can just copy the the grid-mapfile we
    found above (that grep command) onto this path, replacing the
    default "nimbus-grid-mapfile"
</p>

<p>
    In the cloud configuration you
    will see that there is a handy way to make the grid-mapfile just a basic
    entry barrier.  The real authorization decision can be made by more fine
    grained policies on a per user basis (for example, limiting certain users to
    certain amounts of total VM time, etc.).
</p>


<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<a name="part-IIc"> </a>
<h3>II.C. Test call _NAMELINK(part-IIc)</h3>

<p>
    If you used the auto-configuration program, you are
    not ready for a live test yet.  You will come back to this section
    after configuring workspace-control.  <a href="#part-III">Jump there now</a>.
</p>
<p>
    If you did <i>not</i> follow the auto-configuration program, currently Nimbus is
    set to "fake" mode.  This allows you to get the service and VMM nodes
    working independently before you "join them up" for the live end to end
    test.
</p>

<a name="new-services"> </a>
<h4>* New services: _NAMELINK(new-services)</h4>

<p>
    Starting the container again will result in these new services:
</p>

<div class="screen"><pre>https://10.20.0.1:8443/wsrf/services/ElasticNimbusService
https://10.20.0.1:8443/wsrf/services/WorkspaceContextBroker
https://10.20.0.1:8443/wsrf/services/WorkspaceEnsembleService
https://10.20.0.1:8443/wsrf/services/WorkspaceFactoryService
https://10.20.0.1:8443/wsrf/services/WorkspaceGroupService
https://10.20.0.1:8443/wsrf/services/WorkspaceService
https://10.20.0.1:8443/wsrf/services/WorkspaceStatusService</pre>
</div>


<a name="test-client"> </a>
<h4>* Client: _NAMELINK(test-client)</h4>

<p>
    In another terminal, set up the environment variables for <i>GLOBUS_LOCATION</i>
    and <i>X509_CERT_DIR</i> (as needed) and run this program:
</p>

_EXAMPLE_CMD_BEGIN
cd $GLOBUS_LOCATION
_EXAMPLE_CMD_END

_EXAMPLE_CMD_BEGIN
./bin/workspace
_EXAMPLE_CMD_END


<div class="screen"><pre>Problem: You must supply an action.
See help (-h).</pre>
</div>

<p>
    OK, let's check out help, then.
</p>

_EXAMPLE_CMD_BEGIN
./bin/workspace -h
_EXAMPLE_CMD_END

<p>
    See sample output <a href="workspace-client-help.txt">here</a>.
</p>

<p>
    A lot of options.  This is the scriptable reference client.  The cloud
    client is more user friendly.  The cloud configuration which supports
    the cloud client is the recommended setup to provide users access to.
    This is very much because the cloud client offers a low entry barrier to
    people that just want to start getting work done.
</p>

<p>
    For this quickstart we are going to run two of the actions listed in help,
    <i>--deploy</i> and <i>--destroy</i>.
    You can experiment with other ones, each action has its own help section.
</p>

_NAMELINK(deploy-test)
<h4>* Deploy test: _NAMELINK(deploy-test)</h4>

<p>
    Grab this <a href="test-create.sh">test script</a>.
</p>

_EXAMPLE_CMD_BEGIN
wget http://www.nimbusproject.org/docs/?doc=2.2/admin/test-create.sh
_EXAMPLE_CMD_END

<p>
    Run it:
</p>

_EXAMPLE_CMD_BEGIN
sh test-create.sh
_EXAMPLE_CMD_END

<p>
    Sample successful output is <a href="test-create-output.txt">here</a>
</p>

<p>
    As you can see, an IP address was allocated to the VM, a schedule given,
    and then some state changes reported.
</p>
<p>
    If you opened a third terminal to destroy, you could see the state change
    move after the destruction, too.  Or you could type <i>CTRL-C</i> to exit
    this command and run destroy in the same terminal.
</p>

_EXAMPLE_CMD_BEGIN
$GLOBUS_LOCATION/bin/workspace -e test.epr --destroy
_EXAMPLE_CMD_END     

<p>
    ... and we get something like:
</p>

<div class="screen"><pre>Destroying workspace 2 @ "https://10.20.0.1:8443/wsrf/services/WorkspaceService"... destroyed.</pre>
</div>

<p>
    If you look at this file, you will now see some usage recorded:
</p>

_EXAMPLE_CMD_BEGIN
cat $GLOBUS_LOCATION/var/nimbus/accounting-events.txt
_EXAMPLE_CMD_END


<p>
    The "CREATED" line is a record of the deployment launch.  A reservation
    for time is made.
</p>

<p>
    The "REMOVED" line is a record of the destruction.  A recording of the
    actual time used is made.  These actual usage recordings stay long term
    in an internal accounting database and (along with any current reservations)
    can be used to make authorization decisions on a per-user basis.
</p>



<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->




<br />

<a name="part-III"> </a>
<h2>Part III: Install/verify workspace-control _NAMELINK(part-III)</h2>

<ul style="list-style: disc">
    <li>III.A. <a href="#part-IIIa">Download and install</a></li>
    <li>III.B. <a href="#part-IIIb">Necessary configurations</a></li>
    <li>III.C. <a href="#part-IIIc">Testing</a></li>
</ul>


<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->

<a name="part-IIIa"> </a>
<h3>III.A. Download and install _NAMELINK(part-IIIa)</h3>

<p>
    Download the "Control Agents" tar file from the download page, and untar it.
</p>

<p>
    This quickstart assumes you are using "/opt/nimbus" as the target directory
    of the install.  You need root privileges to complete the installation.
</p>

<a name="create-priv-user-and-group"> </a>
<h4>* Create privileged user: _NAMELINK(create-priv-user)</h4>
        
<p>
    First, you need to choose (or create) a user that will be
    used to run the backend script (for this guide, let's assume that your
    user is called nimbus). Since we do not allow the backend
    script to be run as root, this user will rely on sudo to run Xen commands
    and other privileged commands.
</p>

<p>
    Next, as root you should change permissions like so:
</p>

_EXAMPLE_ROOTCMD_BEGIN
cd /opt/nimbus
_EXAMPLE_CMD_END

_EXAMPLE_ROOTCMD_BEGIN
chown -R root bin etc lib libexec src
_EXAMPLE_CMD_END

_EXAMPLE_ROOTCMD_BEGIN
chown -R nimbus var
_EXAMPLE_CMD_END

_EXAMPLE_ROOTCMD_BEGIN
find . -type d -exec chmod 775 {} \;
_EXAMPLE_CMD_END

_EXAMPLE_ROOTCMD_BEGIN
find . -type f -exec chmod 664 {} \;
_EXAMPLE_CMD_END

<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->

<a name="part-IIIb"> </a>
<h3>III.B. Necessary configurations _NAMELINK(part-IIIb)</h3>


<h4>* Configure sudo:</h4>

<p>
    Using the visudo command, add the sudo policies printed out by the
    installer to the /etc/sudoers file. These policies should look
    something like this.
</p>

<div class="screen"><pre>
nimbus ALL=(root) NOPASSWD: /opt/nimbus/libexec/workspace-control/mount-alter.sh
nimbus ALL=(root) NOPASSWD: /opt/nimbus/libexec/workspace-control/dhcp-config.sh
nimbus ALL=(root) NOPASSWD: /opt/nimbus/libexec/workspace-control/xen-ebtables-config.sh
</pre>
</div>

<p>
    These policies reflect the user that will be running workspace control
    (nimbus) and
    the correct full paths to the libexec tools.  See 
    "/opt/nimbus/etc/workspace-control/sudo.conf" for more information
</p>

<p>
    Also note that there is a specific ebtables script for xen and kvm.
</p>

<p>
    You may need to comment out any "requiretty" setting in the sudoers policy:
</p>

<div class="screen"><pre>
#Defaults    requiretty
</pre>
</div>

<p>
    The commands run via sudo are not using a terminal and so if you have
    "requiretty" enabled, this can cause a failure.
</p>

<a name="dhcp-conf"> </a>
<h4>* Configure DHCP: _NAMELINK(dhcp-conf)</h4>

<p>
    DHCP is used here as a delivery mechanism only, these DHCP servers do NOT
    pick the addresses to use on their own. Their policy files are dynamically
    altered by workspace-control as needed. Policy additions include the MAC
    addresses which is used to make sure the requester receives the intended
    DHCP lease.
</p>

<p>
    Configuring the DHCP server consists of copying the example DHCP file
    "dhcp.conf.example" (included in the "share/workspace-control" directory) to
    "/etc/dhcp/dhcpd.conf" and editing it to include the proper subnet lines
    (see the contents of the example file). The subnet lines are necessary
    to get the DHCP server to listen on the node's network interface. So,
    make sure that you add a subnet line that matches the subnet of the
    node's network interface. No lease configurations, available ranges, etc.
    should be added: these are added dynamically to the file after the token
    at the bottom.
</p>

<p>
    In most cases it is unecessary, but if you have a non-standard DHCP
    configuration you may need to look at the "dhcp-config.sh" script in the
    protected workspace bin directory and look at the "adjust as necessary"
    section. The assumptions made are as follows:
</p>

<ul>
    <li>DHCP policy file to adjust: "/etc/dhcp/dhcpd.conf"</li>
    <li>Stop DHCP server: "/etc/init.d/dhcpd stop"</li>
    <li>Start DHCP server: "/etc/init.d/dhcpd start"</li>
    <li>The standard unix utility "dirname" is assumed to be
        installed. This is used to find the workspace-control
        utilities "dhcp-conf-alter.py" and "ebtables-config.sh",
        we assume they are in the same directory as "dhcp-config.sh"
        itself. Paths to these can alternatively be hardcoded to fit
        your preferred configuration.</li>
</ul>

<p>
    The "aux/foreign-subnet" script (in the workspace control
    source directory) may be needed for DHCP support. It allows
    VMMs to deliver IP information over DHCP to workspaces even if the VMM
    itself does not have a presence on the target IP's subnet. This is an
    advanced configuration, you should read through the script's leading
    comments and make sure to clear up any questions before using. It is
    particularly useful for hosting workspaces with public IPs where the VMMs
    themselves do not have public IPs. This is because it does not require
    a unique interface alias for each VMM (public IPs are often scarce
    resources).
</p>


<a name="kernel-conf"> </a>
<h4>* Configure kernel(s): _NAMELINK(kernel-conf)</h4>

<p>
    Copy any kernels you wish to use to the <i>/opt/nimbus/var/workspace-control/kernels</i>
    directory, and list them in the <i>authz_kernels</i> option in the <i>kernels.conf</i> configuration file.
    By doing this, clients can choose from these kernels in the metadata,
    but they must already exist at the hypervisor node and must be in the
    guestkernels list.
</p>

<a name="net-conf"> </a>
<h4>* Configure network(s): _NAMELINK(net-conf)</h4>

<p>
    In the workspace-control <i>networks.conf</i> configuration
    file, find notes about specifying the bridge name to use.
</p>



<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->

<a name="part-IIIc"> </a>
<h3>III.C. Testing _NAMELINK(part-IIIc)</h3>

<p>
    For testing with a real VM (see testing section below) using that
    <a href="test-create.sh">test-create.sh</a> script, add your test VM
    from the <a href="#part-Ib">Xen</a> section.
</p>

<p>
    The script is expecting a file named "ttylinux-xen" in the
    <i>/opt/nimbus/var/workspace-control/images</i> directory because of the
    <i>file://ttylinux-xen</i> line in the xml definition file the
    script points to
    (<i>$GLOBUS_LOCATION/share/nimbus-clients/sample-workspace.xml</i>).
</p>




<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<br />

<a name="part-IV"> </a>
<h2>Part IV: End to end test _NAMELINK(part-IV)</h2>

<p>
    Now revisit the service <a href="#part-IIc">Test call</a> section.
</p>

<p>
    If you did <i>not</i> use the <a href="#part-IIb">auto-configuration</a> program:
</p>

<p>
   You are now ready to turn "fake" mode off and try a real VM launch.  Back
   on the Nimbus services node:
</p>

_EXAMPLE_CMD_BEGIN
nano -w $GLOBUS_LOCATION/etc/nimbus/workspace-service/other/common.conf
_EXAMPLE_CMD_END

<p>
    ... and change the "fake.mode" setting to "false"
</p>


<div class="screen"><pre>
fake.mode=false
</pre></div>

<p>
    Now revisit the service <a href="#part-IIc">Test call</a> section.
</p>


<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->


<br />
<br />

<hr />
<hr />

<p>
    Do not hesitate to contact the
    <a href="_NIMBUS_WEBSITE/contact/">workspace-user</a> mailing list with
    problems.
</p>
<p>
    We plan to streamline some of the steps and also significantly add to the
    troubleshooting and reference sections.
</p>

<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />

_NIMBUS_CENTER2_COLUMN_END
_NIMBUS_FOOTER1
_NIMBUS_FOOTER2
_NIMBUS_FOOTER3
